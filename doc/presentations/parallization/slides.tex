% Here is the URL of the package being
% used to process these slides:
% http://prosper.sourceforge.net/

\documentclass[pdf, nototal, slideBW]{prosper}
% Other good style is corners, fyma

\usepackage[dvips]{graphics}
\usepackage{color}
\usepackage{epsfig}

% Automagic full screen
%\hypersetup{pdfpagemode=FullScreen}

\newcommand{\insfig}[2]{
  \begin{center}
    \scalebox{#1}{
      \input{#2}
      }
  \end{center}
}

\newcommand{\insgraphic}[2]{
  \begin{center}
    \scalebox{#1}{
      \includegraphics{#2}
    }
  \end{center}
}

\title{A hybrid parallized FDTD code}

%\subtitle{Applications and Analysis}

\author{Matt Hughes}
\email{mhughe@uvic.ca}
\date{July 28rd, 2004}
\institution{University of Victoria}
\slideCaption{\normalsize Parallel FDTD}

%\Logo(10,0){\scalebox{0.4}{\includegraphics{multiheadsm.eps}}}

\begin{document}

\maketitle

\begin{slide}{Summary}

  \begin{itemize}
    \item Code Overview
    \item Memory Organization Improvements
    \item Minerva
    \item Hybrid Parallelization
    \item Compiler comparison
    \item Python Scripting
    \item The Future
    \item Conclusion
  \end{itemize}

\end{slide}

\begin{slide}{Advantages}
  Introducing Phred
  \begin{itemize}
  \item Fast
  \item Portable; builds and runs on
    \begin{itemize}
    \item Apple Macintosh G4, G5 Mac OS X
    \item Intel/AMD Linux (possibly Windows with cygwin)
    \item IBM RS/6000 (Minerva) AIX
    \item Probably SGI IRIX
    \end{itemize}
  \item Documented with Doxygen
  \end{itemize}
\end{slide}

\begin{slide}{Architecture}
  Goals:
  \begin{itemize}
  \item Performance
  \item Modularity
  \item Safety (sanity checks)
  \item Readability
  \end{itemize}

\end{slide}

\begin{slide}{Language: C++}
  Object Oriented Programming
  \begin{itemize}
  \item Data and the functions that operate on that data are
    grouped into classes
  \item Polymorphism
  \item C++ standard library: safe, easy to use data structures
  \item Templates can be used to gain OO flexibility without the cost
    of virtual function calls
  \item Metaprogramming - code that generates code
  \end{itemize}  
  No high performance code (update loops) uses function calls of any
  kind. 
\end{slide}

\begin{slide}{A brief example}
\tiny
\begin{verbatim}
class Excite {
  virtual float excite(int timestep) = 0;
};

class Gaussm : public Excite {
  float df_, f0;
  virtual float excite(int timestep) 
  { return exp(-pow((t-4. / (PI * df_)) * df * PI, 2)
           * sin(2. * PI * fO * (t - 4. / (PI * df))); }
};

void main() {
  Excite *e = new Gaussm();
  float output = e->excite(1);
}
\end{verbatim}
\end{slide}

\begin{slide}{Well defined interfaces}
  Interfaces are provided for a number of important things:
  \begin{itemize}
  \item Grids
  \item Excitations
  \item Boundary Conditions
  \item Results
  \item FDTD Controller 
  \end{itemize}
  
  Any number of subclasses can be created to add new behaviors...
\end{slide}

\begin{slide}{FDTD Controllers}
  FDTD Controller objects
  \begin{itemize}
  \item Stores a Grid object
  \item Stores lists of excitations to apply
  \item Stores lists of results to calculate
  \item Some know how to load data files to build these lists
  \end{itemize}

  \insgraphic{1}{fdtd-classes.eps}
  
\end{slide}

\begin{slide}{Grid Classes}
  \begin{itemize}
  \item Store grid field component data
  \item Knows how to update the field components
  \item Allows the use of the simplest, fastest possible update
    equations 
  \end{itemize}

  $\rightarrow$ Try new things out without breaking what works

  \insgraphic{0.75}{grid-classes.eps}
\end{slide}

\begin{slide}{Boundary Condition Classes}
  \begin{itemize}
  \item Implement update equations for boundaries
  \item Can have non-zero thickness
    \begin{itemize}
    \item Any region claimed by a boundary condition is not updated by
      the grid
    \end{itemize}
  \end{itemize}
  
\insgraphic{0.5}{boundary-classes.eps}

\end{slide}

\begin{slide}{DataWriters and Results}
  DataWriter classes:
  \begin{itemize}
  \item Write N dimensional array data in a given file format
  \end{itemize}

  \vspace{0.5cm}

  Result classes:
  \begin{itemize}
  \item Compute a specific result 
  \end{itemize}
  
  A well defined interface exists for passing data from Results to
  DataWriters
  \insgraphic{0.5}{datawriter-classes.eps}

\end{slide}

%% \begin{slide}{Yee Cell}
%%   Yee Cell field components as held in memory at $(i,j,k)$
  
%%   \insgraphic{0.75}{yee-cell.eps}

%% \end{slide}

\begin{slide}{Flow of execution}
  \begin{enumerate}
  \item Load material information, structure, excitations, required
    outputs
  \item Initialize data structures, allocate memory
  \item Run FDTD time stepping:
    \begin{enumerate}
    \item Update H field components
    \item Apply H excitations
    \item Apply H boundary conditions
    \item Update E field components
    \item Apply E excitations
    \item Apply E boundary conditions
    \item Write results
    \end{enumerate}
  \item De-initialize, deallocate memory
  \end{enumerate}
\end{slide}

\begin{slide}{Memory Organization - Old way}
  % Picture representing old memory organization
  \insgraphic{0.8}{jan-memory.eps}
\end{slide}

\begin{slide}{Memory Organization - New way}
  \insgraphic{0.8}{phred-memory.eps}

  Memory offset calculation: $idx = k + (j + i * n_y) * n_z$
\end{slide}

\begin{slide}{Memory Organization Performance}
  1 Million nodes (100x100x100), 100 time steps
  \vspace{0.5cm}

  AMD Athlon 1800+ (1152 MHz), 256 KB Cache, 512 MB RAM
  \vspace{0.5cm}
  
  10 Trials each
  \vspace{0.5cm}

  Jan's average time: 27.66 seconds\\
  Phred's average time: 18.76 seconds
  \vspace{0.5cm}

  $\approx 32\%$ speed increase
  \vspace{0.5cm}

  Code is simplified, memory usage reduced
\end{slide}

\begin{slide}{Minerva Supercomputer}
  % Picture of minerva / 
  \insgraphic{0.5}{minerva-sp.eps}
  
  \begin{itemize}
  \item 8 node IBM RS/6000 SP 
  \item 16 375 MHz 64-bit Processors per node
  \item 8 Gb Memory per node (2 Gb accessible in 32-bit mode)
  \item Nodes connected by a high performance communication fabric (500 MB/s)
  \item 4 nodes assigned to serial jobs
  \item 4 nodes assigned to parallel jobs
  \end{itemize}
\end{slide}

\begin{slide}{Parallelization Techniques}
  \begin{itemize}
  \item OpenMP (Shared Memory SMP)
  \item Message Passing Interface (MPI)
  \item Vector processors
    \begin{itemize}
    \item Intel SSE2
    \item IBM/Motorola/Apple AltiVec (a.k.a. Velocity Engine)
    \item Cray (and other) Supercomputers
    \end{itemize}
  \end{itemize}

  MPI + OpenMP $\rightarrow$ Hybrid Parallelization
\end{slide}

\begin{slide}{OpenMP: Symmetrical Multiprocessing}
  \begin{itemize}
  \item Simple to use - compiler does most of the work
  \item Fast - all threads run on the same machine and share memory;
    no network latency incurred
  \item Strange problems can occur
  \end{itemize}
  \vspace{0.5cm}

  It is possible to use from 1 to 16 processors at a time on each
  Minerva node
  \begin{itemize}
  \item Each processor is assigned a region of the grid to work on
  \item Division of labor is along x axis, so each thread accesses
  contiguous regions of memory $\rightarrow$ cache 
  \item All processors access the same memory 
  \end{itemize}

\end{slide}

\begin{slide}{OpenMP Example}
\begin{verbatim}
int a[] = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1};
#pragma omp parallel for
for (i = 0; i < 10; i++)
  a[i] = a[i] + 1;  
\end{verbatim}
  If two threads were assigned to this loop, the first would handle
  the first 5 elements of the array, the second the other 5 elements. 
\end{slide}

\begin{slide}{Message Passing Interface}

  \begin{itemize}
  \item The FDTD domain is decomposed into an even number of
    sub-domains.
  \item Adjacent sub-domains overlap by two cells
  \item Each sub-domain is assigned to a Minerva node.
  \item Sub-domains and nodes are completely independent during update loops
  \end{itemize}

  \insgraphic{0.5}{domain-decomp.eps}
  
\end{slide}

\begin{slide}{Overlap detail}
  Sub-domain boundaries are treated as boundary conditions
  \begin{itemize}
  \item Data is exchanged by a class derived from Boundary
  \item Any object can ask that data be exchanged between adjacent
    sub-domains (PML and UPML for instance)
  \end{itemize}

  \insgraphic{0.75}{overlap.eps}
\end{slide}

\begin{slide}{MPI Derived Datatypes}  
  \begin{itemize}
    \item An easy way to access non-contiguous regions of memory
    \item Powerful tool for moving data
  \end{itemize}

  Ex: Extracting a column from a matrix
  \insgraphic{0.75}{mpi-datatypes.eps}
  Start $= 3$, stride $= 3$, block size $=1$, num blocks $= 4$
\end{slide}

\begin{slide}{Parallel Performance Scaling - Real time}
  100x100x100 Grid, 1000 time steps, ewalls

  \insgraphic{0.5}{realtime.eps}
\end{slide}

\begin{slide}{Parallel Performance Scaling - Speed up}
  100x100x100 Grid, 1000 time steps, ewalls

  \insgraphic{0.5}{speedup.eps}
\end{slide}

\begin{slide}{Parallel Performance Scaling - Real time}
  500x100x100 Grid, 1000 time steps, ewalls

  \insgraphic{0.5}{realtime-5m.eps}
\end{slide}

\begin{slide}{Parallel Performance Scaling - Speed up}
  500x100x100 Grid, 1000 time steps, ewalls

  \insgraphic{0.5}{speedup-5m.eps}
\end{slide}

\begin{slide}{Scaling problems and solutions}
   Parallel scaling is not as good as it might be, especially for more
   than 5 million nodes
  \begin{itemize}
  \item MPI network connection bandwidth?
  \item Memory bandwidth?
  \item Cache problems?
  \end{itemize}

  More investigation and experimentation is required
\end{slide}

\begin{slide}{Compiler Comparison}
  100x100x100 cells, 100 time steps \\
  Programs were compiled with -O2 only

  \vspace{0.5cm}
  Tests run on Koala\\
  AMD Athlon XP 2000+ stepping 2\\
  1666.388 MHZ,   1.5 Gb RAM, 256 KB Cache\\

  \vspace{0.5cm}
  Intel 8.0 20031016 Compiled binary: 13.966 sec\\
  GCC 3.2 20020903 Compiled binary: 16.017 sec\\

  \vspace{0.5cm}
  Intel's compiler produces binaries that are $\approx 12.81\%$ \\
  faster than those generated by GCC.  
\end{slide}

\begin{slide}{Python scripting language}
  A Python interpreter can optionally be linked to Phred

  \begin{itemize}
  \item Full power of Python available
  \item Access to external data
  \item Write alternative script loaders
  \item Problem set up can be done in Python
  \item Optimization Loops
  \item X window display 
  \end{itemize}

  Not yet available on Minerva (compiler bug)
\end{slide}

\begin{slide}{The Future}
  General Code
  \begin{itemize}
  \item Performance, optimization
  \item Improved parallel scaling
  \item Auditing for safety, sanity checks, memory leaks
  \end{itemize}

  \vspace{0.5cm}
  \begin{itemize}
  \item Fix UPML
  \item Near to Farfield
  \item S parameter measurements
  \item Better plane wave excitation
  \item etc
  \end{itemize}
\end{slide}

\begin{slide}{Conclusion}
  \begin{itemize}
  \item Promising parallel FDTD code has been development
  \item Large problems (at least 20 million cells) are solvable in a
  reasonable time frame
  \item Flexible Python scripting language
  \item OO Design makes extension, experimentation, and enhancement easy 
  \end{itemize}
\end{slide}

\begin{slide}{?}
\huge
\vspace{1cm}
Questions?
\end{slide}

\end{document}
